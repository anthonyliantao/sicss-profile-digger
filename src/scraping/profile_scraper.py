from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import requests
import re
import os
import csv
from pathlib import Path

def sanitize_filename(name: str) -> str:
    # æ›¿æ¢éžå­—æ¯æ•°å­—ä¸ºä¸‹åˆ’çº¿
    return re.sub(r"[^\w\-]+", "_", name).strip("_")

# åˆ›å»ºç›®å½•
def ensure_dir(path: str | Path):
    Path(path).mkdir(parents=True, exist_ok=True)

# èŽ·å–ç½‘é¡µ HTML å†…å®¹
def fetch_html(url: str, headless: bool = True) -> str:
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=headless)
        page = browser.new_page()
        page.goto(url)
        page.wait_for_load_state("networkidle")
        html = page.content()
        browser.close()
    return html

# è§£æžå•ä¸ªäººå‘˜åŒºå—
def parse_profile_block(block, base_url: str, save_photo: bool, image_dir: str | Path):
    # å§“å
    name_tag = block.select_one("h5.font-weight-bold")
    name = name_tag.get_text(strip=True) if name_tag else None

    # ç®€ä»‹
    body_div = block.select_one("div.media-body")
    for h5 in body_div.select("h5"):
        h5.decompose()
    bio = body_div.get_text(separator=" ", strip=True)

    # å›¾ç‰‡
    img_tag = block.select_one("img")
    photo_url = urljoin(base_url, img_tag["src"]) if img_tag else None
    photo_path = None

    if photo_url and save_photo:
        safe_name = sanitize_filename(name)
        photo_path = os.path.join(image_dir, f"{safe_name}.jpg")
        try:
            r = requests.get(photo_url, timeout=10)
            r.raise_for_status()
            with open(photo_path, "wb") as f:
                f.write(r.content)
        except Exception as e:
            print(f"âš ï¸ Failed to download image: {photo_url} â†’ {e}")
            photo_path = None

    return {
        "name": name,
        "bio": bio,
        "photo_path": photo_path
    }

# è§£æžæ•´ä¸ªé¡µé¢ä¸­çš„ profile å—
def parse_profiles(html: str, base_url: str, save_photo: bool, image_dir: str | Path) -> list[dict]:
    soup = BeautifulSoup(html, "html.parser")
    all_profiles = []
    
    # ðŸ” æå–æ—¥æœŸä¸Žåœ°ç‚¹ï¼ˆåªæå–ç¬¬ä¸€ä¸ªåŒ¹é…ï¼‰
    info_tag = soup.select_one("p.h4.text-light")
    date, location = None, None
    if info_tag:
        text = info_tag.get_text(strip=True)
        if "|" in text:
            date, location = map(str.strip, text.split("|", maxsplit=1))
        else:
            date = text  # fallback æƒ…å†µ    

    # éåŽ†æ¯ä¸ª role å—
    for h3 in soup.select("h3.h3.mb-4"):
        role = h3.get_text(strip=True)

        # æ”¶é›†è¯¥ h3 åˆ°ä¸‹ä¸€ä¸ª h3 ä¹‹é—´çš„æ‰€æœ‰ profile å—
        profile_blocks = []
        for sibling in h3.find_next_siblings():
            if sibling.name == "h3" and "mb-4" in sibling.get("class", []):
                break  # é‡åˆ°ä¸‹ä¸€ä¸ª role åŒºå—ï¼Œç»“æŸå½“å‰æ”¶é›†
            if sibling.name == "div" and "media" in sibling.get("class", []) and "mb-5" in sibling.get("class", []):
                profile_blocks.append(sibling)

        # è§£æžæ¯ä¸€ä¸ªå—å¹¶åŠ å…¥ role å­—æ®µ
        for block in profile_blocks:
            profile = parse_profile_block(block, base_url, save_photo, image_dir)
            profile["role"] = role
            profile["date"] = date
            profile["location"] = location
            all_profiles.append(profile)

    return all_profiles

# ä¿å­˜ä¸º CSV
def save_to_csv(data: list[dict], output_file: str | Path):
    ensure_dir(Path(output_file).parent)
    with open(output_file, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=["name", "bio", "photo_path", "role", "date", "location"])
        writer.writeheader()
        writer.writerows(data)
    print(f"[âœ…] CSV saved to {output_file}")
    
# ä¸»è°ƒåº¦å‡½æ•°
def scrape_profiles(
    url: str,
    image_dir: str | Path = "data/raw/images",
    output_file: str | Path = "data/raw/profiles.csv",
    headless: bool = True,
    save_photo: bool = True
) -> list[dict]:
    if save_photo:
        ensure_dir(image_dir)

    html = fetch_html(url, headless=headless)
    profiles = parse_profiles(html, url, save_photo, image_dir)

    if output_file:
        save_to_csv(profiles, output_file)

    return profiles